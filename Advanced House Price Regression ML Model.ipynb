{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building Machine Learning Pipeline : Data Analysis Phase\n",
    "\n",
    "#In this we will focus on creating Machine Learning Pipelines considering all the life cycle of a DataScience Projects.\n",
    "\n",
    "# Project name : House Price : Advanced Regession Technique\n",
    "\n",
    "The main aim of this project is to predict the house price based on various features which we will discuss as we go ahead\n",
    "\n",
    "Dataset To be Download Below Link :\n",
    "\n",
    "https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data\n",
    "\n",
    "### All the Lifecycle In A Data Science Projects\n",
    "Data Analysis\n",
    "\n",
    "Feature Engineering\n",
    "\n",
    "Feature Selection\n",
    "\n",
    "Model Building\n",
    "\n",
    "Model Deployment\n",
    "\n",
    "## Data Analysis Phase\n",
    "## MAin aim is to understand more about the data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "## Display all the columns of the dataframe\n",
    "\n",
    "pd.pandas.set_option('display.max_columns',None)\n",
    "\n",
    "dataset=pd.read_csv('train.csv')\n",
    "\n",
    "## print shape of dataset with rows and columns\n",
    "print(dataset.shape)\n",
    "\n",
    "## print the top5 records\n",
    "dataset.head()\n",
    "\n",
    "#### #### In Data Analysis We will Analyze To Find out the below stuff\n",
    "1. Missing Values\n",
    "2. All The Numerical Variables\n",
    "3. Distribution of the Numerical Variables\n",
    "4. Categorical Variables\n",
    "5. Cardinality of Categorical Variables\n",
    "6. Outliers\n",
    "7. Relationship between independent and dependent feature(SalePrice)\n",
    "\n",
    "\n",
    "## Missing Values :\n",
    "\n",
    "## Here we will check the percentage of nan values present in each feature\n",
    "features_with_na=[features for features in dataset.columns if dataset[features].isnull().sum()>1]\n",
    "\n",
    "for feature in features_with_na:\n",
    "    print(feature, np.round(dataset[feature].isnull().mean(), 3),  ' % Missing Values')\n",
    "\n",
    "##### Since There Are many Missing Values , We need To find the Reletionship between Missin values and Sales Price\n",
    "\n",
    "\n",
    "# let's Plot soe Daigram for The Relestionship\n",
    "\n",
    "for feature in features_with_na:\n",
    "    data = dataset.copy()\n",
    "    \n",
    "    data[feature] = np.where(data[feature].isnull(), 1, 0)\n",
    "    \n",
    "    # calculate the mean saleprice where the information is missing\n",
    "    data.groupby(feature)['SalePrice'].median().plot.bar()\n",
    "    plt.title(feature)\n",
    "    plt.show()\n",
    "\n",
    "Note : Here We Clearly See The Reletionship Between Missing Vslues and Dependent Variable. So we need to replace missing values with some meaninngfull var which we do in feature Engineering section.\n",
    "\n",
    "\n",
    "## Numerical Variables\n",
    "\n",
    "numerical_features = [feature for feature in dataset.columns if dataset[feature].dtypes != 'O']\n",
    "print('Number of numerical variable : ', len(numerical_features))\n",
    "\n",
    "#visualize the numerical variables\n",
    "dataset[numerical_features].head()\n",
    "\n",
    "Temoproal Variable (Datetime variable)\n",
    "\n",
    "# List of Variable contain year information\n",
    "year_feature = [feature for feature in numerical_features if 'Yr' in feature or 'Year' in feature ]\n",
    "year_feature\n",
    "\n",
    "## Lets analyze the Temporal Datetime Variables\n",
    "## We will check whether there is a relation between year the house is sold and the sales price\n",
    "\n",
    "dataset.groupby('YrSold')['SalePrice'].median().plot()\n",
    "plt.xlabel('Year Sold')\n",
    "plt.ylabel('Median House Price')\n",
    "plt.title(\"House Price vs Year Sold\")\n",
    "\n",
    "## Hear we will compare the difference between all years feature with SalePrice\n",
    "\n",
    "for feature in year_feature:\n",
    "    if feature!='YrSold':\n",
    "        data=dataset.copy()\n",
    "        ## We will capture the difference between year variable and year the house was sold for\n",
    "        data[feature]=data['YrSold']-data[feature]\n",
    "\n",
    "        plt.scatter(data[feature],data['SalePrice'])\n",
    "        plt.xlabel(feature)\n",
    "        plt.ylabel('SalePrice')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "## Numerical variables are usually of 2 type\n",
    "## 1. Continous variable and Discrete Variables\n",
    "\n",
    "discrete_feature=[feature for feature in numerical_features if len(dataset[feature].unique())<25 and feature not in year_feature+['Id']]\n",
    "print(\"Discrete Variables Count: {}\".format(len(discrete_feature)))\n",
    "\n",
    "discrete_feature\n",
    "\n",
    "## Lets Find the realtionship between them and Sale PRice\n",
    "\n",
    "for feature in discrete_feature:\n",
    "    data=dataset.copy()\n",
    "    data.groupby(feature)['SalePrice'].median().plot.bar()\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('SalePrice')\n",
    "    plt.title(feature)\n",
    "    plt.show()\n",
    "\n",
    "### Continuous Variable\n",
    "\n",
    "continuous_feature=[feature for feature in numerical_features if feature not in discrete_feature+year_feature+['Id']]\n",
    "print(\"Continuous feature Count {}\".format(len(continuous_feature)))\n",
    "\n",
    "## Lets analyse the continuous values by creating histograms to understand the distribution\n",
    "\n",
    "for feature in continuous_feature:\n",
    "    data=dataset.copy()\n",
    "    data[feature].hist(bins=25)\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(feature)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## we will be using Logarithmic Transformation\n",
    "\n",
    "for feature in continuous_feature:\n",
    "    data=dataset.copy()\n",
    "    if 0 in data[feature].unique():\n",
    "        pass\n",
    "    else:\n",
    "        data[feature]=np.log(data[feature])\n",
    "        data['SalePrice']=np.log(data['SalePrice'])\n",
    "        plt.scatter(data[feature],data['SalePrice'])\n",
    "        plt.xlabel(feature)\n",
    "        plt.ylabel('SalesPrice')\n",
    "        plt.title(feature)\n",
    "        plt.show()\n",
    "\n",
    "## outliers\n",
    "\n",
    "for feature in continuous_feature:\n",
    "    data=dataset.copy()\n",
    "    if 0 in data[feature].unique():\n",
    "        pass\n",
    "    else:\n",
    "        data[feature]=np.log(data[feature])\n",
    "        data.boxplot(column=feature)\n",
    "        plt.ylabel(feature)\n",
    "        plt.title(feature)\n",
    "        plt.show()\n",
    "        \n",
    "    \n",
    "\n",
    "## Categorical Variables\n",
    "\n",
    "categorical_features=[feature for feature in dataset.columns if data[feature].dtypes=='O']\n",
    "categorical_features\n",
    "\n",
    "dataset[categorical_features].head()\n",
    "\n",
    "for feature in categorical_features:\n",
    "    print('The feature is {} and number of categories are {}'.format(feature,len(dataset[feature].unique())))\n",
    "\n",
    "## Find out the relationship between categorical variable and dependent feature SalesPrice\n",
    "\n",
    "for feature in categorical_features:\n",
    "    data=dataset.copy()\n",
    "    data.groupby(feature)['SalePrice'].median().plot.bar()\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('SalePrice')\n",
    "    plt.title(feature)\n",
    "    plt.show()\n",
    "\n",
    "# Advanced Housing Prices - Feature Engineering\n",
    "\n",
    "The Main aim of the poject is to predict house price based on given features.\n",
    "\n",
    "dataset To be downloaded from the below link\n",
    "\n",
    "https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data\n",
    "\n",
    "We will be performing all the below steps in Feature Engineering\n",
    "\n",
    "Missing values\n",
    "Temporal variables\n",
    "Categorical variables: remove rare labels\n",
    "Standarise the values of the variables to the same range\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# to visualize all the columnns inn the dataframe\n",
    "pd.pandas.set_option('display.max_columns', None)\n",
    "\n",
    "## Merge Train And Test dataset Fo Feature Enginering And Scaling\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "dataset_mrg = pd.concat([train_df,test_df])\n",
    "\n",
    "dataset_mrg.shape\n",
    "\n",
    "dataset = pd.DataFrame(dataset_mrg)\n",
    "\n",
    "dataset.head()\n",
    "dataset.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Create Data Splitting in train and test data \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(dataset,dataset['SalePrice'],test_size=0.1,random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape\n",
    "\n",
    "### Missing Values\n",
    "\n",
    "features_nan = [feature for feature in dataset.columns if dataset[feature].isnull().sum()>1 and dataset[feature].dtypes=='O']\n",
    "\n",
    "for  feature in features_nan:\n",
    "    print(\"{}:{}% missing values\".format(feature,np.round(dataset[feature].isnull().mean(),3)))\n",
    "\n",
    "## Replace missing value with a new label\n",
    "def replace_cat_feature(dataset,features_nan):\n",
    "    data=dataset.copy()\n",
    "    data[features_nan]=data[features_nan].fillna('Missing')\n",
    "    return data\n",
    "\n",
    "dataset=replace_cat_feature(dataset,features_nan)\n",
    "\n",
    "dataset[features_nan].isnull().sum()\n",
    "\n",
    "dataset.head(10)\n",
    "\n",
    "## Now lets check for numerical variables the contains missing values\n",
    "numerical_with_nan=[feature for feature in dataset.columns if dataset[feature].isnull().sum()>1 and dataset[feature].dtypes!='O']\n",
    "\n",
    "## We will print the numerical nan variables and percentage of missing values\n",
    "\n",
    "for feature in numerical_with_nan:\n",
    "    print(\"{}: {}% missing value\".format(feature,np.around(dataset[feature].isnull().mean(),3)))\n",
    "\n",
    "## Replacing the numerical Missing Values\n",
    "\n",
    "for feature in numerical_with_nan:\n",
    "    ## We will replace by using median since there are outliers\n",
    "    median_value=dataset[feature].median()\n",
    "    #print(median_value)\n",
    "    \n",
    " ## create a new feature to capture nan values\n",
    "    dataset[feature+'_nan']=np.where(dataset[feature].isnull(),1,0)\n",
    "    dataset[feature].fillna(median_value,inplace=True)\n",
    "    \n",
    "dataset[numerical_with_nan].isnull().sum()\n",
    "    \n",
    "\n",
    "dataset.head(8)\n",
    "\n",
    "## Temporal Variables (Date Time Variables)\n",
    "\n",
    "for feature in ['YearBuilt','YearRemodAdd','GarageYrBlt']:\n",
    "    dataset[feature]=dataset['YrSold']-dataset[feature]\n",
    "\n",
    "dataset.head()\n",
    "\n",
    "dataset[['YearBuilt','YearRemodAdd','GarageYrBlt']].head()\n",
    "\n",
    "import numpy as np\n",
    "num_features=['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea']\n",
    "\n",
    "for feature in num_features:\n",
    "    dataset[feature]=np.log(dataset[feature])\n",
    "\n",
    "dataset.head()\n",
    "\n",
    "## Handlinng Rare Categorical Feature\n",
    "\n",
    "we will remove categorical variabe that are present less then 1% of the Observations\n",
    "\n",
    "categorical_features=[feature for feature in dataset.columns if dataset[feature].dtype=='O']\n",
    "\n",
    "categorical_features\n",
    "\n",
    "for feature in categorical_features:\n",
    "    temp=dataset.groupby(feature)['SalePrice'].count()/len(dataset)\n",
    "    temp_df=temp[temp>0.01].index\n",
    "    dataset[feature]=np.where(dataset[feature].isin(temp_df),dataset[feature],'Rare_var')\n",
    "    \n",
    "    \n",
    "\n",
    "dataset.head(50)\n",
    "\n",
    "scaling_feature=[feature for feature in dataset.columns if feature not in ['Id','SalePerice'] ]\n",
    "len(scaling_feature)\n",
    "\n",
    "scaling_feature\n",
    "\n",
    "\n",
    "\n",
    "for feature in categorical_features:\n",
    "    labels_ordered=dataset.groupby([feature])['SalePrice'].mean().sort_values().index\n",
    "    labels_ordered={k:i for i,k in enumerate(labels_ordered,0)}\n",
    "    dataset[feature]=dataset[feature].map(labels_ordered)\n",
    "    \n",
    "\n",
    "dataset.head()\n",
    "\n",
    "# Feature Scaling\n",
    "\n",
    "feature_scale=[feature for feature in dataset.columns if feature not in ['Id','SalePrice']]\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler()\n",
    "scaler.fit(dataset[feature_scale])\n",
    "\n",
    "scaler.transform(dataset[feature_scale])\n",
    "\n",
    "# transform the train and test set, and add on the Id and SalePrice variables\n",
    "data = pd.concat([dataset[['Id', 'SalePrice']].reset_index(drop=True),\n",
    "                    pd.DataFrame(scaler.transform(dataset[feature_scale]), columns=feature_scale)],\n",
    "                    axis=1)\n",
    "\n",
    "data.head()\n",
    "\n",
    "data.to_csv('new_dataset.csv',index=False)\n",
    "\n",
    "# Feature Selection Advanced House Price Prediction\n",
    "\n",
    "The main aim of project is to predict the house price based on various features\n",
    "\n",
    "### Dataset to be Downloaded from the below link:\n",
    "\n",
    "\n",
    "https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "## for feature selection\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# to visualize all the columns in the dataframe\n",
    "pd.pandas.set_option('display.max_columns',None)\n",
    "\n",
    "dataset=pd.read_csv(\"new_dataset.csv\")\n",
    "\n",
    "\n",
    "dataset=dataset.drop_duplicates()\n",
    "dataset = dataset.drop(['SalePrice_nan'],axis=1)\n",
    "dataset.head(2122)\n",
    "\n",
    "np.any(np.isnan(dataset))\n",
    "\n",
    "np.all(np.isfinite(dataset))\n",
    "\n",
    "np.where(np.isnan(dataset))\n",
    "\n",
    "dataset =dataset.fillna(dataset.mean())\n",
    "\n",
    " # set dependent variable\n",
    "y_train =dataset[['SalePrice']]\n",
    "\n",
    "y_train.head()\n",
    "\n",
    "## drop dependent feature from dataset\n",
    "X_train=dataset.drop(['Id','SalePrice'],axis=1)\n",
    "X_train.head()\n",
    "\n",
    "\n",
    "\n",
    "### Apply Feature Selection\n",
    "# first, I specify the Lasso Regression model, and I\n",
    "# select a suitable alpha (equivalent of penalty).\n",
    "# The bigger the alpha the less features that will be selected.\n",
    "\n",
    "# Then I use the selectFromModel object from sklearn, which\n",
    "# will select the features which coefficients are non-zero\n",
    "\n",
    "feature_sel_model = SelectFromModel(Lasso(alpha=0.005, random_state=0))\n",
    "feature_sel_model.fit(X_train, y_train)\n",
    "\n",
    "feature_sel_model.get_support()\n",
    "\n",
    "# let's print the number of total and selected features\n",
    "\n",
    "# this is how we can make a list of the selected features\n",
    "selected_feat = X_train.columns[(feature_sel_model.get_support())]\n",
    "\n",
    "# let's print some stats\n",
    "print('total features: {}'.format((X_train.shape[1])))\n",
    "print('selected features: {}'.format(len(selected_feat)))\n",
    "\n",
    "\n",
    "selected_feat\n",
    "\n",
    "X_train=X_train[selected_feat]\n",
    "X_train.head()\n",
    "\n",
    "X_train.shape\n",
    "\n",
    "\n",
    "\n",
    "y_train.to_csv('y_train')\n",
    "\n",
    "X_train.to_csv('X_train')\n",
    "\n",
    "# Reression Model : Advance House Price Prediction\n",
    "\n",
    "After completing Featuer Engineering and Feature selection , Now We Create Regression Model TO Predict House Price.\n",
    "\n",
    "## Import Libraiers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import mean_squared_error as MSE \n",
    "\n",
    "# import dataset train And Test\n",
    "X = pd.read_csv('X_train')\n",
    "y = pd.read_csv('y_train')\n",
    "\n",
    "y\n",
    "\n",
    "y = y.drop(['Unnamed: 0'],axis=1)\n",
    "X= X.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "X.head()\n",
    "\n",
    "## Train_Test_split Dataset\n",
    "\n",
    "## Create Data Splitting in train and test data \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y, test_size=0.6,random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "### Prediction And Selecting Alorithem\n",
    "\n",
    "import xgboost\n",
    "regressor=xgboost.XGBRegressor()\n",
    "\n",
    "booster=['gbtree','gblinear']\n",
    "base_score=[0.25,0.5,0.75,1]\n",
    "\n",
    "## Hyper Parameter Optimization\n",
    "\n",
    "n_estimators = [100,500,900,1100,1500]\n",
    "max_depth = [2,3,5,10,15]\n",
    "booster = ['gbtree','gblinear']\n",
    "learning_rate = [0.05, 0.1, 0.15, 0.20]\n",
    "min_child_weight=[1,2,3,4]\n",
    "\n",
    "# Define Hyperparameter to Search\n",
    "hyperparameter_grid ={\n",
    "     'n_estimators': n_estimators,\n",
    "    'max_depth':max_depth,\n",
    "    'learning_rate':learning_rate,\n",
    "    'min_child_weight':min_child_weight,\n",
    "    'booster':booster,\n",
    "    'base_score':base_score\n",
    "}\n",
    "\n",
    "# Set up the random search with 4-fold cross validation\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "random_cv = RandomizedSearchCV(estimator=regressor,\n",
    "            param_distributions=hyperparameter_grid,\n",
    "            cv=5, n_iter=50,\n",
    "            scoring = 'neg_mean_absolute_error',n_jobs = 4,\n",
    "            verbose = 5, \n",
    "            return_train_score = True,\n",
    "            random_state=42)\n",
    "\n",
    "random_cv.fit(X_train,y_train)\n",
    "\n",
    "random_cv.best_estimator_\n",
    "\n",
    "regressor=xgboost.XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=1, gamma=0, learning_rate=0.05, max_delta_step=0,\n",
    "       max_depth=15, min_child_weight=4, missing=None, n_estimators=900,\n",
    "       n_jobs=4, nthread=None, objective='reg:linear', random_state=0,\n",
    "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "       silent=True, subsample=1)\n",
    "\n",
    "regressor.fit(X_train,y_train)\n",
    "\n",
    "import pickle\n",
    "filename = 'Ad_House_price_model.pkl'\n",
    "pickle.dump(regressor, open(filename, 'wb'))\n",
    "\n",
    "# Predict the model \n",
    "y_pred = regressor.predict(X_test) \n",
    "\n",
    "y_pred\n",
    "\n",
    "##Create Sample Submission file and Submit using ANN\n",
    "pred=pd.DataFrame(y_pred)\n",
    "sub_df=pd.read_csv('sample_submission.csv')\n",
    "datasets=pd.concat([sub_df['Id'],pred],axis=1)\n",
    "datasets.columns=['Id','SalePrice']\n",
    "datasets.to_csv('sample_submission.csv',index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Completed####"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
